{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtRH-J0QHaJ5"
      },
      "outputs": [],
      "source": [
        "# ðŸ§± Agent: Analyze and generate signals based on the fetched data for multiple tokens, considering sentiment-weighted total\n",
        "def analyze_multiple_tokens_data_with_sentiment(all_tokens_data):\n",
        "    # Dictionary to store processed data for each token\n",
        "    all_tokens_analysis = {}\n",
        "\n",
        "    # Loop through each token's data\n",
        "    for token, combined_df in all_tokens_data.items():\n",
        "        print(f\"Processing data for {token}...\")\n",
        "\n",
        "        # Calculate daily returns based on the 'value_y' column (price data)\n",
        "        combined_df['return'] = combined_df['value_y'].pct_change() * 100\n",
        "\n",
        "        # Identify social volume spikes using 'value_x' for social volume\n",
        "        combined_df['rolling_avg'] = combined_df['value_x'].rolling(window=7).mean()\n",
        "\n",
        "        # Buy signal: when social volume spikes above 1.5x the rolling average and sentiment-weighted total is positive\n",
        "        combined_df['buy_signal'] = (combined_df['value_x'] > (combined_df['rolling_avg'] * 1.5)) & (combined_df['value'] > 0)\n",
        "\n",
        "        # Sell signal: when social volume drops below the rolling average, return is negative, and sentiment-weighted total is negative\n",
        "        combined_df['sell_signal'] = (combined_df['value_x'] < combined_df['rolling_avg']) & (combined_df['return'] < 0) & (combined_df['value'] < 0)\n",
        "\n",
        "        # Drop rows with missing values (NaN)\n",
        "        combined_df = combined_df.dropna()\n",
        "        combined_df.reset_index(inplace=True)  # This will turn the index into a regular column\n",
        "\n",
        "        # Store the processed data for this token\n",
        "        all_tokens_analysis[token] = combined_df\n",
        "\n",
        "    return all_tokens_analysis\n"
      ]
    }
  ]
}